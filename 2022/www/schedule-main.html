<h3>Schedule</h3>
      <p>This schedule is subject to change. Edits will be ongoing, but you can assume that all updates
      for the subsequent week will be completed by 5p Chicago Local Time of the Friday 
      prior to the week of instruction.</p>
      <p>
      <b>Readings:</b> Readings are from:

      Please read the posted material and watch the corresponding videos for
      that day before coming to the class meeting.  </p>
      <p>
      <b>Course videos:</b> All videos for the course are currently available
      on Box, with links on the course Canvas page. </p>

 <table class="table table-striped table-custom">
   <tr>
     <th class="custom-col-1">Date</th>
     <th class="custom-col-2">Topic</th>
     <th class="custom-col-3">Reading</th>
     <th class="custom-col-4">Lab (Fridays)</th>
     <th class="custom-col-5">Notes</th>
   </tr>



             <tr>
                <td>30 Mar</td>
                <td>Introduction</td>
                <td>1 (I), 1 (H)</td>
                <td></td>
                <td></td>
            </tr>
            

             <tr>
                <td>1 Apr</td>
                <td>Data: From Sources to Feautres</td>
                <td>2 (H)</td>
                <td>Lab 1: Data</td>
                <td></td>
            </tr>
            

             <tr>
                <td>6 Apr</td>
                <td>Prediction, Training, Loss</td>
                <td>2.3 (E), Breiman</td>
                <td></td>
                <td></td>
            </tr>
            

             <tr>
                <td>8 Apr</td>
                <td>Evaluation: RSS, Accuracy, ROC</td>
                <td>2.2 (I), 3 (H),</td>
                <td>Lab 2: Linear Regression</td>
                <td></td>
            </tr>
            

             <tr>
                <td>13 Apr</td>
                <td>Model Selection: Cross-Validation, Grid Search</td>
                <td>5.1 (I)</td>
                <td></td>
                <td>Project Proposal Due</td>
            </tr>
            

             <tr>
                <td>15 Apr</td>
                <td>Linear Models I: Linear Methods</td>
                <td>3.1-3.2 (I)</td>
                <td></td>
                <td></td>
            </tr>
            

             <tr>
                <td>20 Apr</td>
                <td>Linear Models II: Features, Basis Functions</td>
                <td>3.3 (I), 7.1-7.3 (I)</td>
                <td></td>
                <td></td>
            </tr>
            

             <tr>
                <td>22 Apr</td>
                <td>Linear Models III: Bias-Variance, Regularization</td>
                <td>4 (H)</td>
                <td></td>
                <td></td>
            </tr>
            

             <tr>
                <td>27 Apr</td>
                <td>Logistic Regression</td>
                <td>4.3 (I)</td>
                <td>Lab 3: Classification</td>
                <td></td>
            </tr>
            

             <tr>
                <td>29 Apr</td>
                <td>Support Vector Machines</td>
                <td>9 (I), 5 (H)</td>
                <td></td>
                <td></td>
            </tr>
            

             <tr>
                <td>4 May</td>
                <td>Kernel Smoothing: KDE, Naive Bayes</td>
                <td>6.1-6.2 (Optional),6.6</td>
                <td></td>
                <td></td>
            </tr>
            

             <tr>
                <td>6 May</td>
                <td>Tree-Based Methods</td>
                <td>8.1 (I), 6 (H)</td>
                <td>Lab 4: Trees and Forests</td>
                <td></td>
            </tr>
            

             <tr>
                <td>11 May</td>
                <td>Ensemble Learning</td>
                <td>8.2 (I), 7 (H)</td>
                <td></td>
                <td></td>
            </tr>
            

             <tr>
                <td>13 May</td>
                <td>Dimensionality Reduction: SVD, PCA</td>
                <td>8 (H), 10.1-10.2 (I)</td>
                <td></td>
                <td></td>
            </tr>
            

             <tr>
                <td>18 May</td>
                <td>Non-Parametric Clustering : K-Means, Hierarchical</td>
                <td>9 (H)</td>
                <td></td>
                <td></td>
            </tr>
            

             <tr>
                <td>20 May</td>
                <td>Parametric Clustering: KDE, GMM, DBSCAN, Meanshift</td>
                <td>10.3 (I)</td>
                <td>Lab 5: Clustering</td>
                <td></td>
            </tr>
            

             <tr>
                <td>25 May</td>
                <td>Fairness Metrics</td>
                <td>Roth, Mitchell</td>
                <td></td>
                <td></td>
            </tr>
            

             <tr>
                <td>27 May</td>
                <td>Ethics of Big Data/Machine Larning</td>
                <td>Salganik</td>
                <td></td>
                <td></td>
            </tr>
            
           
</table>
