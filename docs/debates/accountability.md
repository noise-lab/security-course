## AI Accountability

### Resolution

**Companies deploying AI systems should be held legally liable for any harms caused by their technologies.**

The debate will be "Oxford Style" and follow [this format](format.md).

### Resources

Here are some resources and questions to help guide and nuance the debate:

**Questions**

1. How should we define and measure "harm" caused by AI systems?
2. What is the appropriate standard of liability: strict liability, negligence, or something else?
3. How do we attribute responsibility when AI systems involve multiple parties (developers, deployers, users)?
4. Should liability depend on whether the AI system was deployed in a high-risk domain (e.g., healthcare, criminal justice)?
5. How does AI liability affect innovation and the willingness of companies to deploy beneficial AI systems?
6. What role should transparency and explainability play in determining liability?

**Readings**

- [Regulating AI: Accountability, Transparency, and Liability](https://www.oecd.org/going-digital/ai/accountability-in-ai.pdf)
- [AI Accountability in the Private Sector](https://www.wired.com/story/ai-accountability-is-everyones-business/)
- [Corporate Responsibility in the Age of AI](https://hbr.org/2020/11/building-the-ai-powered-organization)
- [Holding AI Systems Accountable](https://www.weforum.org/agenda/2022/01/ai-accountability-ethical-guidelines/)
- [AI Ethics and Accountability in Industry](https://www.ibm.com/blogs/research/2020/11/ai-ethics-global-challenge/)
- [EU AI Act - Risk-Based Approach to AI Regulation](https://artificialintelligenceact.eu/)
